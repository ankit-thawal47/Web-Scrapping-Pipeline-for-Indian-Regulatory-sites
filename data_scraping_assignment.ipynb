{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To scrap all the pdfs from CCI website\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import datetime\n",
    "import csv\n",
    "from PyPDF2 import PdfFileReader #used the PyPDF2 < 3.0\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) \n",
    "#TO SUPPRESS THE SSL WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL VARS\n",
    "\n",
    "#This sets hold all the relevant URLs of particular domains\n",
    "global_set_of_urls = set()\n",
    "global_set_of_pdf_urls = set()\n",
    "\n",
    "# Names of the Output Folders\n",
    "output_folder = \"txt_files\"\n",
    "pdf_folder_for_sebi = \"sebi_pdfs\"\n",
    "pdf_folder_for_cci = \"cci_pdfs\"\n",
    "pdf_folder_for_rbi = \"rbi_pdfs\"\n",
    "\n",
    "#dataframe to save the extracted pdf\n",
    "df = pd.DataFrame(columns=[\"file_text\"])\n",
    "\n",
    "# List of URLs\n",
    "\n",
    "# 1. Home Page for Competition Commission of India\n",
    "cci_homepage_url = \"https://www.cci.gov.in/\"\n",
    "\n",
    "# 2. Home Page for Securities and Exchange Board of India (SEBI)\n",
    "# Note: The homepage URL for SEBI has been changed. I have included only the Menu's as all the relevant info/links are there.\n",
    "sebi_homepage_url = \"https://www.sebi.gov.in/js/menu.js\"\n",
    "\n",
    "# 3. Home Page for RBI\n",
    "rbi_homepage_url = \"https://www.rbi.org.in/\"\n",
    "\n",
    "# 4. Home Page of indiacode\n",
    "indiacode_initializer_url = \"https://www.indiacode.nic.in/handle/123456789/1362/browse?type=actyear\" #instead of using the homepage, I am using the page where all the acts are listed by year\n",
    "\n",
    "\n",
    "#Excluded words from the urls\n",
    "excluded_suffixes_for_urls = [\"pdf\", \"xlsx\", \"img\", \"docx\", \"png\"]\n",
    "excluded_words_from_urls_of_cci = [\"events\", \"gallery\", \"ccijoclp\"]\n",
    "excluded_words_from_urls_of_indiacode = [\"events\", \"gallery\"]\n",
    "excluded_words_from_urls_of_sebi = [\"events\", \"gallery\"]\n",
    "excluded_words_from_urls_of_rbi = [\"events\", \"gallery\"]\n",
    "\n",
    "\n",
    "excluded_urls_of_indiacode = [\n",
    "    \"https://www.indiacode.nic.in/handle/123456789/1362/browse?type=shorttitle\",\n",
    "    \"https://www.indiacode.nic.in/handle/123456789/1362/browse?type=actno\",\n",
    "    \"https://www.indiacode.nic.in/handle/123456789/1362/browse?type=dateissued\",\n",
    "    \"https://www.indiacode.nic.in/handle/123456789/1362/browse?type=ministry\",\n",
    "    \"https://www.indiacode.nic.in/handle/123456789/1362/browse?type=department\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_excluded_word(input_string, excluded_words):\n",
    "    for word in excluded_words:\n",
    "        if word.lower() in input_string.lower():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get all the URLs of the mentioned URL\n",
    "# Function -> to get all urls of CCI into a global set\n",
    "def get_sub_urls_of_cci(url):\n",
    "    # Note : CCI's website constantly giving SSL Error, hence I had to turn off their SSL Verification. Not a good solutions, need to work on that.\n",
    "    read = requests.get(url,verify=False)\n",
    "    html_content = read.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    all_anchors_tags = soup.find_all('a')\n",
    "    for link in all_anchors_tags:\n",
    "        href_link = link.get('href')\n",
    "        #the following if creates a set of urls which starts with http and are the links within the domain of \"cci\"(and not links of twitter, insta etc.)\n",
    "        # Need to save all these strings into lists -> for better readability\n",
    "        if href_link!=None and href_link.lower().startswith(\"http\"):  \n",
    "            if \"cci\" in href_link and not any(href_link.lower().endswith(suffix) for suffix in excluded_suffixes_for_urls) and not (contains_excluded_word(href_link,excluded_words_from_urls_of_cci)):\n",
    "                    global_set_of_urls.add(href_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function -> to get all urls of SEBI into a global set\n",
    "# SEBI Website has external js scripts which holds URL(inside anchor tags), hence need to take that into consideration\n",
    "def get_sub_urls_of_sebi(url):\n",
    "    read = requests.get(url,verify=False)\n",
    "    html_content = read.text\n",
    "    # Note : Need to add Exception handling to make it more robust.\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    all_anchors_tags = soup.find_all('a')\n",
    "    all_javascript_tags = soup.find_all('script')\n",
    "    # To get all the relevant links from the <a> tags\n",
    "    for link in all_anchors_tags:\n",
    "        href_link = link.get('href')\n",
    "        #the following if creates a set of urls which starts with http and are the links within the domain of \"cci\"(and not links of twitter, insta etc.)\n",
    "        if(href_link!=None and href_link.startswith(\"https://\")):\n",
    "            if not (href_link.startswith(\"https://www.sebi\")):\n",
    "                continue\n",
    "        else:\n",
    "            href_link = urljoin(sebi_homepage_url,href_link)\n",
    "        if href_link!=None and href_link.lower().startswith(\"http\"):  \n",
    "            if \"sebi\" in href_link and not any(href_link.lower().endswith(suffix) for suffix in excluded_suffixes_for_urls) and not (contains_excluded_word(href_link,excluded_words_from_urls_of_sebi)):\n",
    "                global_set_of_urls.add(href_link)\n",
    "    # For sebi we need to get a tags as well as <script> tags as, javascript files\n",
    "    for link in all_javascript_tags:\n",
    "        src_link = link.get('src')\n",
    "        if(src_link!=None and src_link.startswith(\"https://\")):\n",
    "            if not (src_link.startswith(\"https://www.sebi\")):\n",
    "                continue\n",
    "        else:\n",
    "            src_link = urljoin(sebi_homepage_url,src_link)\n",
    "        if src_link!=None and src_link.lower().startswith(\"http\"):  \n",
    "            if \"sebi\" in src_link and not any(src_link.lower().endswith(suffix) for suffix in excluded_suffixes_for_urls) and not (contains_excluded_word(src_link,excluded_words_from_urls_of_sebi)):\n",
    "                global_set_of_urls.add(src_link)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_urls_of_rbi(url):\n",
    "    read = requests.get(url,verify=False)\n",
    "    html_content = read.text\n",
    "    # Note : Need to add Exception handling to make it more robust.\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    all_anchors_tags = soup.find_all('a')\n",
    "    \n",
    "    # To get all the relevant links from the <a> tags\n",
    "    for link in all_anchors_tags:\n",
    "        href_link = link.get('href')\n",
    "        #the following if creates a set of urls which starts with http and are the links within the domain of \"cci\"(and not links of twitter, insta etc.)\n",
    "        if(href_link!=None and href_link.startswith(\"https://\")):\n",
    "            if not (href_link.startswith(\"https://www.rbi\")):\n",
    "                continue\n",
    "        else:\n",
    "            href_link = urljoin(rbi_homepage_url,href_link)#change the homepage url based on the domain\n",
    "        if href_link!=None and href_link.lower().startswith(\"http\"):  \n",
    "            if \"rbi\" in href_link and not any(href_link.lower().endswith(suffix) for suffix in excluded_suffixes_for_urls) and not (contains_excluded_word(href_link,excluded_words_from_urls_of_rbi)):\n",
    "                global_set_of_urls.add(href_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_urls_of_indiacode(url):\n",
    "    read = requests.get(url,verify=False)\n",
    "    html_content = read.text\n",
    "    # Note : Need to add Exception handling to make it more robust.\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    all_anchors_tags = soup.find_all('a')\n",
    "\n",
    "    indiacode_homepage_url = \"https://www.indiacode.nic.in/\"\n",
    "    #href_link = urljoin(indiacode_homepage_url,href_link)\n",
    "    \n",
    "    # To get all the relevant links from the <a> tags\n",
    "    for link in all_anchors_tags:\n",
    "        href_link = link.get('href')\n",
    "        #the following if creates a set of urls which starts with http and are the links within the domain of \"cci\"(and not links of twitter, insta etc.)\n",
    "        if(href_link!=None and href_link.startswith(\"https://\")):\n",
    "            if not (href_link.startswith(\"https://www.india\")):\n",
    "                continue\n",
    "        else:\n",
    "            href_link = urljoin(indiacode_homepage_url,href_link)#change the homepage url based on the domain\n",
    "        if(any(href_link.lower().endswith(excluded_url) for excluded_url in excluded_urls_of_indiacode)):\n",
    "            continue\n",
    "        if href_link!=None and href_link.lower().startswith(\"http\"):  \n",
    "            if not any(href_link.lower().endswith(suffix) for suffix in excluded_suffixes_for_urls) and not (contains_excluded_word(href_link,excluded_words_from_urls_of_indiacode)):\n",
    "                global_set_of_urls.add(href_link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT relevant web urls\n",
    "\n",
    "#https://www.cci.gov.in/events/All/details/549 => these urls contain pdf with images -> not at all useful\n",
    "#https://ccijournal.in/index.php/ccijoclp/article/view/16 => such links contains the articles which are case studies, book reviews etc. Hence not useful for our purpose\n",
    "#gallery contains images of events => not useful\n",
    "#the urls which ends with pdf, img, xlsx, docx -> NOT NECESSARY AT ALL when we are making global_set_of_sub_urls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective : To create global list of all the URLs in the website\n",
    "# The function recursively iterate over global_set and captures all the weblinks/url from all over the domain\n",
    "def util_to_get_links_of_suburls(choice): \n",
    "    for link in global_set_of_urls.copy():\n",
    "        if link not in visited_url:\n",
    "            try:\n",
    "                if choice == 1:\n",
    "                    get_sub_urls_of_cci(link)#change this based on CCI or SEBI domain\n",
    "                elif choice == 2:\n",
    "                    get_sub_urls_of_sebi(link)\n",
    "                elif choice == 3:\n",
    "                    get_sub_urls_of_rbi(link)\n",
    "                elif choice == 4:\n",
    "                    get_sub_urls_of_indiacode(link)\n",
    "                visited_url.add(link)\n",
    "            except Exception as e:\n",
    "                print(\"exception {e} occured for : {link}\")\n",
    "\n",
    "#print(len(global_set_of_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Method to save all the collected list of Global URLs in CSV File\n",
    "\n",
    "def make_csv_file_from_set(input_set,output_file):\n",
    "    try:\n",
    "        with open(output_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Write the elements from the set(global_set_of_urls) to the CSV file\n",
    "            for item in input_set:\n",
    "                writer.writerow([item])\n",
    "    except Exception as e:\n",
    "        print(\"Exception occured : \",e)\n",
    "    else:\n",
    "        print(\"CSV file created: \", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method that return soup of the url passed as arguments\n",
    "# Update : To handle HttpConnectionTimeout, max tries reached Error => Exception handline as well as HTTPAdapter is added\n",
    "\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "def soup_returner(url):\n",
    "    soup = BeautifulSoup()\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        retry = HTTPAdapter(max_retries=5)\n",
    "        session.mount(\"http://\", retry)\n",
    "        session.mount(\"https://\", retry)\n",
    "        read = session.get(url,verify=False)\n",
    "        html_content = read.text\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "    except Exception as e:\n",
    "        print(\"URL \", url)\n",
    "        print(\"Exception occured : \",e )\n",
    "    return soup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to fetch all the urls that ends with \"pdf\" \n",
    "# The method creates a global_set of all the \"pdf\" urls by using the global_set of urls\n",
    "\n",
    "def make_set_of_pdf_links():\n",
    "\t#traverse one by one from global url set and for each url get the All links of pdf files under that webpage\n",
    "\tfor url in global_set_of_urls:\n",
    "\t\tsoup = soup_returner(url)\n",
    "\t\tonly_anchor_tags = soup.find_all('a')\n",
    "\t\tfor link in only_anchor_tags:\n",
    "\t\t\thref_link = link.get('href')\n",
    "\t\t\t#print(href_link)\n",
    "\t\t\tif href_link!=None and href_link.lower().endswith(\".pdf\"):\n",
    "\t\t\t\tglobal_set_of_pdf_urls.add(href_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# A METHOD TO CONVERT PDF CONTENT TO TEXT ##################################\n",
    "def get_content_from_pdf(pdf_path):\n",
    "    response = requests.get(pdf_path,verify=False)\n",
    "\n",
    "    with io.BytesIO(response.content) as f:\n",
    "        pdfreader = PdfFileReader(f,strict=False)\n",
    "        information = pdfreader.getDocumentInfo()\n",
    "        number_of_pages = pdfreader.getNumPages()\n",
    "\n",
    "        print(information.title)\n",
    "\n",
    "        #following code snippets to convert pdf_url into base64 encoding\n",
    "        def convert_url_to_base64(url):\n",
    "            sample_string = url\n",
    "            sample_string_bytes = sample_string.encode(\"ascii\") \n",
    "            base64_bytes = base64.b64encode(sample_string_bytes) \n",
    "            base64_string = base64_bytes.decode(\"ascii\")\n",
    "            return base64_string\n",
    "\n",
    "        filename = convert_url_to_base64(pdf_path)\n",
    "        relative_path= os.path.join(output_folder,filename)\n",
    "        all_extracted_content = \"\"\n",
    "\n",
    "        # for page in range(0,number_of_pages):\n",
    "        #     page_text = pdfreader.getPage(page)\n",
    "        #     all_extracted_content += page_text.extractText()\n",
    "\n",
    "        # with open (relative_path+\".txt\", 'w', encoding=\"utf-8\") as file:\n",
    "        #     file.write(all_extracted_content)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Method to download all the pdf using the global_set of pdfs\n",
    "# Each domain will have seperate folder directory\n",
    "# Note : Need to automate changing the folder names according to the domain\n",
    "\n",
    "# Note : Handle if the folder is not already created, it should also create it. \n",
    "\n",
    "def download_pdfs(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, verify=False)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "        # Extract the file name from the URL\n",
    "        filename = \"\"\n",
    "        pdf_file_name = \"\"\n",
    "        with io.BytesIO(response.content) as f:\n",
    "            pdfreader = PdfFileReader(f,strict=False)\n",
    "            information = pdfreader.getDocumentInfo()\n",
    "            \n",
    "            file_text = \"\"\n",
    "\n",
    "            for page_num in range(pdfreader.getNumPages()):\n",
    "                file_text += pdfreader.getPage(page_num).extractText()\n",
    "            new_index = len(df)\n",
    "            df.loc[new_index, 'file_text'] = file_text\n",
    "\n",
    "        print(f\"Written data frame for : \")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing frame {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Method to extract text from scanned documents\n",
    "# The method returns title and the text content of the pdf\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def extract_text_and_title(pdf_path):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "        # Iterate through each page\n",
    "        text_of_entire_file = \"\"\n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_number)\n",
    "\n",
    "            # Convert the page to an image\n",
    "            image = page.get_pixmap()\n",
    "            img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "\n",
    "            # Perform OCR on the image\n",
    "            text = pytesseract.image_to_string(img)\n",
    "            text_of_entire_file += text\n",
    "            # Get the title (you may need to adjust this based on your PDF structure)\n",
    "            title = page.get_text(\"title\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text and title: {e}\")\n",
    "\n",
    "    return title,text_of_entire_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m visited_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m() \u001b[38;5;66;03m#to keep track of urls already visited\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mutil_to_get_links_of_suburls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#3. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#4. Save this list of global_set of URLS in csv file -> just for a backup\u001b[39;00m\n\u001b[0;32m     14\u001b[0m globalurls_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCCIglobalURL.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;66;03m#output file to save the set of all global URLs\u001b[39;00m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mutil_to_get_links_of_suburls\u001b[1;34m(choice)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mget_sub_urls_of_cci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#change this based on CCI or SEBI domain\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     10\u001b[0m         get_sub_urls_of_sebi(link)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mget_sub_urls_of_cci\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sub_urls_of_cci\u001b[39m(url):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Note : CCI's website constantly giving SSL Error, hence I had to turn off their SSL Verification. Not a good solutions, need to work on that.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     read \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m read\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      7\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1368\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:317\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:278\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 278\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "choice = 1\n",
    "#Choices for different website\n",
    "#1. CCI Website\n",
    "#2. SEBI\n",
    "#3. RBI\n",
    "#4. IndiaCode\n",
    "\n",
    "if choice == 1:\n",
    "    #Step by step scraping pipeling for CCI\n",
    "    #1. Add initializer page to global_set\n",
    "    print(\"###################### INITIALIZING ###################################\")\n",
    "    print(\"1. Creating a global set of URLs using HomePage URL of <### CCI ###>\")\n",
    "    global_set_of_urls.add(cci_homepage_url)\n",
    "    #2. Get all the suburls using the initializer url(try to go through it 2 times)\n",
    "    print(\"2. Searching Recursively for URLs\")\n",
    "    visited_url = set() #to keep track of urls already visited\n",
    "    for i in range(0,2):\n",
    "        util_to_get_links_of_suburls(choice)\n",
    "    #3. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "    print(\"3. Removing unimportant urls based on some conditions\")\n",
    "    #4. Save this list of global_set of URLS in csv file -> just for a backup\n",
    "    print(\"4. Save this list of global_set of URLS in csv file -> just for a backup\")\n",
    "    globalurls_filename = \"CCIglobalURL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_urls,globalurls_filename)\n",
    "    #5. Get all the pdf urls using the global_set of urls\n",
    "    print(\"5. Getting all the <pdf urls> using the global_set of urls\")\n",
    "    global_set_of_pdf_urls = set()\n",
    "    make_set_of_pdf_links()\n",
    "    #6. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "    print(\"6. Removing unimportant pdf urls based on some conditions\")\n",
    "    #7. Save the list of global_set_of_pdf_urls into a csv file -> just for a backup\n",
    "    print(\"7. Saving the list of global_set_of_pdf_urls into a csv file -> just for a backup\")\n",
    "    globalpdfurls_filename = \"CCIglobalPDF_URL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_pdf_urls,globalpdfurls_filename)\n",
    "    #8. Read each url of pdf link and save its content into a dataframe\n",
    "    print(\"8. Saving the content of each pdf into a dataframe\")\n",
    "    for url in global_set_of_pdf_urls:\n",
    "        download_pdfs(url)\n",
    "    #9. Save this dataframe in csv file\n",
    "    print(\"9. Copying the content into a csv file\")\n",
    "    df.to_csv('Extracted text of CCI PDFs.csv')\n",
    "elif choice == 2:\n",
    "    #1. Add initializer page to global_set\n",
    "    global_set_of_urls.add(sebi_homepage_url)\n",
    "    #2. Get all the suburls using the initializer url(try to go through it 2 times)\n",
    "    visited_url = set() #to keep track of urls already visited\n",
    "    for i in range(0,2):\n",
    "        util_to_get_links_of_suburls(choice)\n",
    "    #3. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "        \n",
    "    #4. Save this list of global_set of URLS in csv file -> just for a backup\n",
    "    globalurls_filename = \"SEBIglobalURL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_urls,globalurls_filename)\n",
    "    #5. Get all the pdf urls using the global_set of urls\n",
    "    global_set_of_pdf_urls = set()\n",
    "    make_set_of_pdf_links()\n",
    "    #6. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "    #7. Save the list of global_set_of_pdf_urls into a csv file -> just for a backup\n",
    "    globalpdfurls_filename = \"SEBIglobalPDF_URL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_pdf_urls,globalpdfurls_filename)\n",
    "    #8. Read each url of pdf link and save its content into a dataframe\n",
    "    for url in global_set_of_pdf_urls:\n",
    "        download_pdfs(url)\n",
    "    #9. Save this dataframe in csv file\n",
    "    df.to_csv('Extracted text of SEBI PDFs.csv')\n",
    "elif choice == 3:\n",
    "    #1. Add initializer page to global_set\n",
    "    global_set_of_urls.add(rbi_homepage_url)\n",
    "    #2. Get all the suburls using the initializer url(try to go through it 2 times)\n",
    "    visited_url = set() #to keep track of urls already visited\n",
    "    for i in range(0,2):\n",
    "        util_to_get_links_of_suburls(choice)\n",
    "    #3. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "        \n",
    "    #4. Save this list of global_set of URLS in csv file -> just for a backup\n",
    "    globalurls_filename = \"RBIglobalURL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_urls,globalurls_filename)\n",
    "    #5. Get all the pdf urls using the global_set of urls\n",
    "    global_set_of_pdf_urls = set()\n",
    "    make_set_of_pdf_links()\n",
    "    #6. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "    #7. Save the list of global_set_of_pdf_urls into a csv file -> just for a backup\n",
    "    globalpdfurls_filename = \"RBIglobalPDF_URL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_pdf_urls,globalpdfurls_filename)\n",
    "    #8. Read each url of pdf link and save its content into a dataframe\n",
    "    for url in global_set_of_pdf_urls:\n",
    "        download_pdfs(url)\n",
    "    #9. Save this dataframe in csv file\n",
    "    df.to_csv('Extracted text of RBI PDFs.csv')\n",
    "elif choice == 4:\n",
    "    #1. Add initializer page to global_set\n",
    "    global_set_of_urls.add(indiacode_initializer_url)\n",
    "    #2. Get all the suburls using the initializer url(try to go through it 2 times)\n",
    "    visited_url = set() #to keep track of urls already visited\n",
    "    for i in range(0,2):\n",
    "        util_to_get_links_of_suburls(choice)\n",
    "    #3. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "        \n",
    "    #4. Save this list of global_set of URLS in csv file -> just for a backup\n",
    "    globalurls_filename = \"IndiaCodeglobalURL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_urls,globalurls_filename)\n",
    "    #5. Get all the pdf urls using the global_set of urls\n",
    "    global_set_of_pdf_urls = set()\n",
    "    make_set_of_pdf_links()\n",
    "\n",
    "    #6. (Optional Preprocessing) Use algo to remove unimportant urls based on some conditions\n",
    "    updated_global_set = set()\n",
    "    for link in global_set_of_pdf_urls:\n",
    "        if \"repealed\" not in link:\n",
    "            indiacode_homepage_url = \"https://www.indiacode.nic.in/\"\n",
    "            new_link = urljoin(indiacode_homepage_url,link)\n",
    "            updated_global_set.add(new_link)\n",
    "\n",
    "    global_set_of_pdf_urls = updated_global_set\n",
    "\n",
    "    #7. Save the list of global_set_of_pdf_urls into a csv file -> just for a backup\n",
    "    globalpdfurls_filename = \"IndiaCodeglobalPDF_URL.csv\"#output file to save the set of all global URLs\n",
    "    make_csv_file_from_set(global_set_of_pdf_urls,globalpdfurls_filename)\n",
    "    #8. Read each url of pdf link and save its content into a dataframe\n",
    "    for url in global_set_of_pdf_urls:\n",
    "        download_pdfs(url)\n",
    "    #9. Save this dataframe in csv file\n",
    "    df.to_csv('Extracted text of IndiaCode PDFs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
